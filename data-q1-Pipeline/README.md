# ขั้นตอนการทำงานของโฟลวจากรูป
1. แหล่งข้อมูล (Data Source) 
ดึงข้อมูลจาก MongoDB
มีทั้งข้อมูล Master (สินค้า, ร้านค้า) และ Transaction (การขาย)
แล้วก็ข้อมูลดิบเป็นsemi-structured

2. การดึงข้อมูล (Ingestion)
้ต้องการรันงานแบบทุก 1 วัน เพื่อดึงข้อมูลออกมาจาก MongoDB
บันทึกไฟล์ลง Google Cloud Storage ก่อน
เก็บใน Raw Zone เป็นไฟล์ JSON แยกตามวันที่

3. การเก็บข้อมูล (Raw Zone)
เราจะเก็บข้อมูลดิบในรูปแบบเปลี่ยนแปลงไม่ได้
จัดเป็นโฟลเดอร์แยกตามวันที่

4. การประมวลผล (Processing)
ใช้ Spark ทำความสะอาดและแปลงข้อมูล แปลง JSON ให้เป็นตาราง
- ลบข้อมูลซ้ำ แก้ค่า null
ส่งผลลัพธ์ออกมาเป็นไฟล์ Parquet แล้วเก็บใน Processed Zone

5. การเก็บข้อมูล (Processed Zone)
เก็บข้อมูลที่ สะอาดและจัดโครงสร้างแล้วตรงนี้จะพร้อมเอาเข้า BigQuery

6. Serving Layer (BigQuery)
ข้อมูลใน BigQuery จัดเป็น 2 ชั้น Staging Tables
- ตารางข้อมูลที่เพิ่งผ่านการ clean 
Mart Tables
ตารางสำหรับการวิเคราะห์
Fact: ธุรกรรม เช่น การขาย
Dimension: รายละเอียดสินค้า ร้านค้า ลูกค้า วันที่

**ข้อมูลสำหรับพร้อมวิเคราะห์ได้เลย

7. Orchestration
Apache Airflow ควบคุม pipeline ทั้งหมดของเส้นที่วางไว้
- ตั้งเวลาให้รันทุกวัน หรือว่าจะกำหนดเปลี่ยนแปลงทีหลังได้ในอนาคต แล้วแต่คนเอาไปใช้
มีการตรวจจับการเปลี่ยน schema แล้วก็แยกเก็บข้อมูลที่โหลดไม่สำเร็จ (Dead-letter bucket)
มีการ alert ถ้าหากว่าเฟล



# ขั้นตอนเบื้องต้นคร่าวๆ สำหรับอ่านประกอบภาพที่แปะไว้นะครับมีประมาณนี้